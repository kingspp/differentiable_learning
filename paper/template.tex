\documentclass{article}

% Package Declarations
\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}         % Lorem Ipsum fill text
\usepackage{multicol}       % Support for Multi columns for tables
\usepackage{multirow}       % Support for Multi rows fot tables
\usepackage{mathtools}      % Advanced mathtools
\usepackage{caption}        % Advanced caption configuration
\usepackage{amsmath}        % Math package for equations       
\usepackage{titlesec}       % Title section
\usepackage{graphicx}       % For adding labels to parts of the equation
\usepackage{stackrel}       % For adding labels to parts of the equation
\usepackage[ruled,vlined]{algorithm2e}  % For Algorithms


% Theme Configurations

% Set section depth to 4
% \setcounter{secnumdepth}{4}

% \titleformat{\paragraph}
% {\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
% \titlespacing*{\paragraph}
% {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% Add padding to text below table
\captionsetup[table]{skip=10pt}

% Configure hat tex
\let\oldhat\hat
\renewcommand{\hat}[1]{\oldhat{\mathbf{#1}}}

% Document Start

\title{Differentiable Learning by means of Neural Network Pruning}

\author{
  Prathyush S Parvatharaju\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Data Science\\
  Worcester Polytechnic University\\
  Worcester, MA 01609 \\
  \texttt{psparvatharaju@wpi.edu} \\
  %% examples of more authors
   \And
 Shreesha Narasimhamurthy \\
  Department of Data Science\\
  Worcester Polytechnic University\\
  Worcester, MA 01609 \\
  \texttt{snarasimhamurthy@wpi.edu} \\
}

\begin{document}
\maketitle

\begin{abstract}
The idea of linear flow i.e, each node in one layer is connected to a certain weight $\hat{W_{ij}}$ to every node in the following layer, for the deep neural network is limiting in the sense of the way we, humans think. The linear flow would be a constraint for a DNN as the later can process data and emulate relationships in higher dimensions. Deeper neural networks are difficult to train and the use of residual learning in the form of short-circuit connections eases the process of training the network. By replacing the linear flow constraint with a decision process of placing short-circuit connections between layers as a gradient-based approach would yield better results. We propose a novel idea of extending the capabilities of previously used short-circuits as differentiable functions, essentially solving “What to feed?”. To tackle the problem, “When to stop?” we propose an algorithm based on Information Transfer derived from differentiable short-circuits. 

In our experiments, we show that increased weighted representation capability results in achieving better accuracy in fewer iterations compared to the standard architecture. We also demonstrate that with 10\% of the previous parameters, the architecture achieves similar results to standard architecture. Another set of experiments shows our proposed architecture builds better representations with minimal data.
\end{abstract}


% keywords can be removed
\keywords{Differentiable Learning \and Pruning \and Neural Architecture Search}


\section{Introduction}
The ability to recognize patterns and develop strong relations among them has made Neural Network supersede state of the art machine learning techniques. The depth of representations is of central importance for many visual recognition tasks. Right from Alex Net \cite{Krizhevsky2012ImageNetCW} up until ResNet \cite{He2016DeepRL} the flow of data followed linear fashion i.e the data flow from one layer to the next layer and so on. ResNet brought in an innovation \emph{" Short Circuits"} meaning there exists a skip connection where the data is fed to the next layer and in certain circumstances, it is fed to the next layer and also the layer after its immediate next. This paved way to train deep-networks as it increased the representational capability of the network by overcoming issues such as vanishing / exploding gradients \cite{Bengio1994LearningLD, Glorot2010UnderstandingTD}. In our work we set out a goal to explore the \emph{"representational capability"} and its effects on the performance of the network. This led to the question - "What to feed?".

ResNet brings in short circuits between residual components and the number of skip connections is restricted. We propose \emph{"Global Short Circuit"}(GSC) to investigate the behavior of short circuits by revoking restriction and providing skip connections from one layer to every other layer. Thereby enhancing the representational capability of the network and therefore yields better accuracy in fewer iterations compared to standard architecture (See section \ref{sssec:ffn}). However, the modification results in a larger network compared to the standard architecture. Due to the increase in parameters, the network is rendered not scalable. In order to tackle this issue this paper proposes an enhanced GSC i.e, \emph{" Differentiable Short Circuit"}(DSC) - a novel architecture to help network scaling by pruning unwanted skip connections over time.

To tackle the issue of \emph{"When to stop"}?, an algorithm - \emph{Information Transfer} is devised. This algorithm is a measure of the amount of flow of data from one layer to another. Keeping track of variance in Information Transfer over iterations, based on a certain threshold a stopping criterion is proposed.

\section{Differentiable Learning}
\label{sec:headings}

Neural networks are differentiable by design. The standard architecture (\ref{sssec:ffn}) is static and does not change over time but, the learning changes over time. There have been efforts towards an automated way of finding the best architecture for the given data - Neural Architecture search \cite{Zoph2016NeuralAS}. The recent advent of DARTS \cite{Liu2019DARTSDA} suggests that a learning based adaptive architecture performs better compared to the static architecture. Given $N$ layers, one needs to identify the optimal data paths. The right set of combinations can be identified by using brute-force by trying out n(n-1) combinations of connection configurations. However, as the depth of the network increases, the search space increases exponentially and training each configuration becomes a tedious and time consuming process. Gradient descent \cite{ruder2016overview} - a first order iterative search space optimization algorithm finds the local minimum of a differentiable function. In order to find the right connections, the  connection is represented as a differentiable function and is optimized using $\frac{\partial loss}{\partial connection}$. Initially, a standard architecture - Feed forward neural network is described and eventually propose the Global short circuit (GSC) and Differentiable Short Circuit (DSC) architectures.

\subsection{Feed Forward Neural Network}
\label{sssec:ffn}
Multilayer perceptron trained using back-propagation \cite{Rumelhart1986LearningIR}, a supervised learning algorithm are useful for their ability to solve the problems stochastically often approximating solutions for complex problems. Acting as a universal approximator, the architecture has a linear flow; i.e, a layer takes the output of the immediate previous layer alone as its input.A densely connected layer provides learning features from all the combinations of the features of the previous layer. With the linear increase in the number of connections (based on the nodes in the layer) the architecture becomes scalable and time efficient. $\hat{Z}$ in Equation \ref{eq:affine_transformation} is the result of an affine transformation and is capable of learning an offset ($\hat{b}$) and a rate of correlation ($\hat{W}$) that fits the given input.

\begin{equation}
\label{eq:affine_transformation}
\begin{aligned}
\hat{Z} &= Input \cdot \hat{W} + \hat{b}&\\
\end{aligned}
\end{equation}

This type of transformation can only learn linear relationships. In order to be capable of learning any non-linearity, a squashing non-linear transformation $\sigma$ \cite{Han1995TheIO} is applied to the output of each node. Sigmoid activation results in a probability as it restricts the range of the output between 0 and 1.

\begin{equation}
\label{eq:sigmoid_activation}
\begin{aligned}
\sigma(x) &= \frac{1}{1+e^{-x}} \;\;\;\;\;\;  x\in\mathbb{R}, 0<\sigma(x)<1
\end{aligned}
\end{equation}

Figure \ref{fig:SimpleFFN.png} represents standard feed forward architecture and equation \ref{eq:ffn_math_representation} is the mathematical formulation of the network. Each node in one layer is connected to a certain weight $\hat{W_{ij}}$ to every node in the following layer. 

\noindent\begin{minipage}{.45\textwidth}
   \centering
   \includegraphics[scale=0.09]{SimpleFFN.png}
   \captionof{figure}{Feed Forward Network}
   \label{fig:SimpleFFN.png}
\end{minipage}
\begin{minipage}{.45\textwidth}
\begin{equation}
\label{eq:ffn_math_representation}
\begin{aligned}
   Dense_{1} &= \sigma(Input \cdot \hat{W}_{1} + \hat{b}_{1}) &\\
   Dense_{2} &= \sigma(Dense_{1} \cdot \hat{W}_{2} + \hat{b}_{2}) &\\
   Dense_{3} &= \sigma(Dense_{2} \cdot \hat{W}_{3} + \hat{b}_{3}) 
\end{aligned}
\end{equation}
\end{minipage}

For the previously mentioned standard network, the number of trainable parameters for a current layer is only a function of the layer's weight, bias and input. It is represented as, 

\begin{equation}
\label{eq:ffn_params}
Params= \sum_{n=1}^{m}
\underbrace{(L_{n-1}\rule[-12pt]{0pt}{5pt}}_{\mbox{input}}
*\underbrace{L_{n})\rule[-12pt]{0pt}{5pt}}_{\mbox{weights}}
+\underbrace{L_{n}\rule[-12pt]{0pt}{5pt}}_{\mbox{bias}}
\end{equation}

For a 3 layer network with node configurations [100,50,10] and 784 input features, using equation \ref{eq:ffn_params} we arrive at 83,550 trainable parameters


\subsection{Global Short Circuit}
This work proposes the Global Short Circuit architecture to explore the impact of skip connection on representational capabilities of the network. There exists a connection from every layer to every other layer in the network. Some of the possible ways of merging layers are explored in DenseNet\cite{Li2018DenselyCC} and ResNet\cite{He2016DeepRL}. In ResNet weighted feature maps are merged using $max$ or $sum$. DenseNet merges feature using a concatenation operation. We define a merge operation similar to DenseNet's $concat$ style merge. However, this type of merging has an issue when it comes to Convolutional Neural Networks where there is a requirement to merge feature maps of different shapes. Hence, this work proposes downsampling/padding operators to get identical shapes. Figure \ref{fig:GSC.png} shows standard architecture defined in section \ref{sssec:ffn} having connections from one layer to every other layer, merged using concat operation. The difference in the connections between the layers is relative to the number of gradient operations required for the respective layer. Equation \ref{eq:gsc_math_representation} provides mathematical representation of the architecture.

\noindent\begin{minipage}{.45\textwidth}
   \centering
   \includegraphics[scale=0.09]{GSC.png}
   \captionof{figure}{Global Short Circuit}
   \label{fig:GSC.png}
\end{minipage}
\begin{minipage}{.45\textwidth}
\begin{equation}
\label{eq:gsc_math_representation}
\begin{aligned}
       Dense_{1} &= \sigma(Input \cdot \hat{W}_{1} + \hat{b}_{1}) &\\
       Dense_{2} &= \sigma([Input \oplus Dense_{1}] \cdot \hat{W}_{2} + \hat{b}_{2}) &\\
       Dense_{3} &= \sigma([Input \oplus Dense_{1} \oplus Dense_{2}] \cdot \hat{W}_{3} + \hat{b}_{3}) 
\end{aligned}
\end{equation}
\end{minipage}

The experiments conducted show that GSC performs better than standard architecture on large and minimal datasets. This is due to increased representational capacity of the network. A layer \emph{n} in the network now has access every other \emph{n-1} previous outputs. This change paves way to create much deeper constructs and complex relationships among the layers. To advocate that the proposed method - GSC creates much deeper constructs than standard architecture, the class confidence of the predictions is measured. Confidence score is unique for each class and the use of \emph{Softmax} function restricts the score between 0 and 1. At the end of each epoch, mean of class confidence score is calculated across a large unseen test set. This measure speaks for the trust in the prediction. Among the variable factors of the measure, the rate of increase and the actual value are vital for the analysis. MNIST \cite{LeCun1998GradientbasedLA}, Fashion-MNIST \cite{Xiao2017FashionMNISTAN}, CIFAR-10 datasets \cite{Krizhevsky2009LearningML} are chosen to account for the variability in the pattern complexity. The training samples are restricted to 10\% of the whole, while still maintaining a large test sample to test the said hypothesis. Figure \ref{fig:cscore_100p_model.png} shows the performance when the architectures across standard and GSC are the same, which leads to a 30\% increase in the trainable parameters due to merge operation. To rule out the difference in the trainable parameters as a reason for achieving high confidence scores, we introduce 10p architecture. In 10p architecture, the architecture across the models vary but the number of trainable parameters are kept constant across the networks. The performance of 10p architectures are shown in Figure \ref{fig:cscore_10p_model.png}.  

% \begin{figure}[h!]
% \centering
% \includegraphics[scale=0.3]{paper/cscore_100p_param.png}
% \caption{10p Data, 100p Model}
% \label{fig:cscore_100p_model}
% \end{figure}


\noindent\begin{minipage}{.5\textwidth}
   \centering
   \includegraphics[scale=0.2]{paper/cscore_100p_param.png}
   \captionof{figure}{10p Data, 100p Model}
   \label{fig:cscore_100p_model.png}
   \centering
   \includegraphics[scale=0.2]{paper/cscore_10p_param.png}
   \captionof{figure}{10p Data, 10p Model}
   \label{fig:cscore_10p_model.png}
\end{minipage}
\begin{minipage}{.4\textwidth}
In figure \ref{fig:cscore_100p_model.png} the difference in the confidence score between that of the standard architecture and GSC is negligable. This is due to the lack of complex patterns across classes in MNIST dataset. Hence, there's a slight difference in the score on Fashion MNIST and a major jump on CIFAR-10. CIFAR-10 comprises of 10 classes of real-world complex object patterns and GSC architecture enables the model to create much deeper constructs compared to the standard architecure. We notice a similar behaviour in the confidence measure across 10p architectures where the number of trainable parameters across the networks is kept constant.
\end{minipage}


However, GSC suffers from an exponential increase in connections resulting in delayed training time. A rise in the number of stacked layers is noticed with the increase in the complexity of the network in terms of depth, which is not scalable. Compared to the standard architecture, GSC has a considerable increase in the number of training parameters. This is due to the effect of merge operation of previous $n-1$ layers for $n$th layer.

\begin{equation}
\label{eq:gsc_params}
Params= \sum_{n=1}^{m}\left[
\underbrace{(\sum_{i=0}^{n}L_{i}\rule[-12pt]{0pt}{5pt}}_{\mbox{merged inputs}}
*\underbrace{L_{n})\rule[-12pt]{0pt}{5pt}}_{\mbox{weights}}
+\underbrace{L_{n}\rule[-12pt]{0pt}{5pt}}_{\mbox{bias}}\right]
\end{equation}

For the aforementioned 3 layer network, an exponential increase of 30\% i.e, 122,750 parameters has to be trained.

\subsection{Differentiable Short Circuit}
This work proposes Differentiable Short Circuit (DSC) to counter the effect of exponential increase in training parameters due to merge operation. It is done so by expressing a connection between layers as a weighted sigmoid gate. We introduce a connection weight $\hat{C}$, a trainable parameter whose gradients are updated by $\frac{\partial loss}{\partial \hat{C}}$. A change in the value of the connection weight affects the loss. Based on this, a gradient descent algorithm can be used to obtain local minima for loss for optimum values of connection weight. The direction and magnitude of the weight affects the performance of the network by introducing unwanted changes in the architecture. In order to check these effects, a transformation function that is capable of restricting the direction and magnitude of the weight in a specified range is used. Neural arithmetic logic unit \cite{Trask2018NeuralAL} propose a layer whose transformation matrix $\hat{W}$ consists just of -1's, 0's and 1's. In order to prevent the layer from changing the scale of the representation of the numbers when mapping the input to the output, a squashing non-linearity ($\sigma$) is applied on just the weights, $\hat{W}$.  Hence, a sigmoid function, defined in equation \ref{eq:sigmoid_activation}, is chosen over a transformation function for $\hat{C}$. As it forces a restriction on the range of output, it also helps in preserving the direction of the input. Figure \ref{fig:DSC.png} shows the use of connection weights applied for the results of concat operation, essentially keeping the connection in check before it is fed to the next layer. The difference in representation of connections between various blocks and the gradient operations shows the number of gradient computations required for the layer or the operation. Equations \ref{eq:dsc_math_representation} provides mathematical representation of the network.

\noindent\begin{minipage}{.45\textwidth}
   \centering
   \includegraphics[scale=0.09]{DSC.png}
   \captionof{figure}{Differentiable Short Circuit}
   \label{fig:DSC.png}
\end{minipage}
\begin{minipage}{.45\textwidth}
\begin{equation}
\label{eq:dsc_math_representation}
\begin{aligned}
   Dense_{1} &= \sigma((\sigma(\hat{C}_{1}) \cdot Input) \cdot \hat{W}_{1} + \hat{b}_{1}) &\\
   Dense_{2} &= \sigma((\sigma(\hat{C}_{2}) \cdot [Input \oplus Dense_{1}]) \cdot \hat{W}_{2} + \hat{b}_{2}) &\\
   Dense_{3} &= \sigma((\sigma(\hat{C}_{3}) \cdot [Input \oplus Dense_{1} \oplus Dense_{2}]) \cdot \hat{W}_{3} + \hat{b}_{3}) 
\end{aligned}
\end{equation}
\end{minipage}

For DSC, we notice a slight increase in parameters compared to GSC's equation \ref{eq:gsc_params} due to the use \emph{gates}. There exists a trainable gate for every connection. The number of parameters for connections is defined by $\sum_{i=0}^{n}L_{i}$. As a result, for the network structure defined in the standard architecture, the number of trainable parameters are 124,418, a rise of 1.3\% over GSC.

\begin{equation}
\label{eq:dsc_params}
Params= \sum_{n=1}^{m}\left[
\underbrace{(\sum_{i=0}^{n}L_{i}\rule[-12pt]{0pt}{5pt}}_{\mbox{merged inputs}}
*\underbrace{L_{n})\rule[-12pt]{0pt}{5pt}}_{\mbox{weights}}
+\underbrace{L_{n}\rule[-12pt]{0pt}{5pt}}_{\mbox{bias}}
+\underbrace{\sum_{i=0}^{n}L_{i}\rule[-12pt]{0pt}{5pt}}_{\mbox{connection weights}}\right]
\end{equation}
The benefit of having trainable gates are of two folds,

1. By keeping track of the variance of $\sigma{(\hat{C})}$ over time has proven to be useful in designing a stopping criterion, elucidated in section \ref{sub:InfoTransfer}

2. Since the output of the gates are probabilistic in nature, we use a threshold function to transform continuous values to a binary mask. The mask is used to prune unwanted skip connections. Section \ref{sec:Pruning} describes the pruning methodology in detail.




\subsubsection{Information Transfer}
\label{sub:InfoTransfer}
This work proposes \emph{Information transfer} (IT) as the percentage of measure of data flow from one layer to the next. The gated connections having sigmoid activation translates to a scaling operation. Each output is scaled based on its significance before the same is fed to the next layer. The scaling transformation is also applied to skip connections as shown in figure \ref{fig:DSC.png}.

\begin{minipage}{.45\textwidth}
    \centering
    \includegraphics[scale=0.13]{SampleTable.png}
    \captionof{figure}{Sample Information Transfer Table}
    \label{fig:DSC.png}
\end{minipage}
\begin{minipage}{.45\textwidth}
    \begin{equation}
    \begin{aligned}
    IT &=\frac{\sum_{c=1}^{N}\hat{W}_{c}}{\sum_{c=1}^{N}|\hat{W}_{c}|}, \\
    \\
    \text{where}~N &= \text{Number of layers,} \\
    \hat{W}_{c} &= \text{Connection weight}\\
    |\hat{W}_{c}| &= \text{Length of connection weight}\\
\end{aligned}
\end{equation}
\end{minipage}


\begin{minipage}{.4\textwidth}
    \lipsum[6]
\end{minipage}
\begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[scale=0.25]{paper/iv.png}
    \captionof{figure}{Information Transfer Rate vs Accuracy}
    \label{fig:iv.png}
\end{minipage}

\subsubsection{Pruning}
\label{sec:Pruning}
To address the issue of exponential increase in trainable parameter due to merge operation in GSC, differentiable short circuit's trainable connection weights are used to selectively prune inessential nodes by removing connections.  If the current input is not important to the task, the value of $\hat{C_{i}}$ will be 0 and the current input information will not be passed on to the next layer. The input to the binary function ranges between 0 and 1. The probability of obtaining 0 or 1 for the binary function can be computed by,
\begin{equation}
\label{eq:ffn_math_representation}
\begin{aligned}
   P(f_{binary(x)}=1) &=x&\\
   P(f_{binary(x)}=0) &=1-P(f_{binary(x)}=1)&\\   
\end{aligned}
\end{equation}
However, the binary connection gate function is not a differentiable function and breaks backpropagation. As a solution, there are generally two solutions. The first is REINFORCE algorithms \cite{Williams1992SimpleSG} and considering its applications, it is computationally expensive and the reward is difficult to design to approximate the gradients. In recent times, few gradient estimators are proposed and among them, the Straight through gradient estimator \cite{Bengio2013EstimatingOP} is found to be useful due to its reduced computational complexity. The function approximates the step function by the identity when computing gradients during the backward pass:
\begin{equation}
\frac{\partial{f_{binaray}(x)}}{\partial x}=1
\end{equation}
With the implementation of straight through estimator, the model parameters can be trained to optimize the use of connection weights $\hat{C}$ with standard backpropagation without defining any additional supervision or reward signal. This assures that the computation complexity does not increase.

\noindent\begin{minipage}{.5\textwidth}
   \centering
   \includegraphics[scale=0.1]{paper/Pruning1.png}
   \captionof{figure}{10p Data, 100p Model}
%   \label{fig:cscore_100p_model.png}
\end{minipage}
\begin{minipage}{.4\textwidth}
\centering
   \includegraphics[scale=0.09]{paper/Pruning2.png}
   \captionof{figure}{10p Data, 10p Model}
%   \label{fig:c.score_10p_model.png}
\end{minipage}



\noindent\begin{minipage}{.5\textwidth}
  \begin{algorithm}[H]
\SetAlgoLined
\KwResult{Write here the result }
 initialization\;
 \While{While condition}{
  instructions\;
  \eIf{condition}{
   instructions1\;
   instructions2\;
   }{
   instructions3\;
  }
 }
 \caption{Dependency based pruning}
\end{algorithm}

\end{minipage}
\begin{minipage}{.4\textwidth}
\lipsum[6]
\end{minipage}







% \begin{figure}[h!]
% \centering
% \includegraphics[scale=0.1]{Pruning.png}
% \caption{Pruning}
% \label{fig:infotransfer}
% \end{figure}

\section{Experiments}
\label{sec:others}
\lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures}
\lipsum[10] 
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11] 

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
%   \label{fig:SimpleFFN.png}
\end{figure}

\subsection{Tables}
\lipsum[12]
See awesome Table~\ref{tab:table}.

% \begin{table}
%  \caption{Results}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Model     & Parameters     & Dataset ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
%   \label{tab:table}
% \end{table}

% Please add the following required packages to your document preamble:
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table}[]
\centering
\begin{tabular}{@{}llllllll@{}}
\toprule
\multirow{2}{*}{Model}      & \multirow{2}{*}{Parameter} & \multirow{2}{*}{Dataset} & \multicolumn{2}{l}{Train}       & \multicolumn{2}{l}{Test} & \multirow{2}{*}{Time Elapsed} \\ \cmidrule(lr){4-7}
                            &                            &                          & Accuracy @ Steps & Loss @ Steps & Accuracy      & Loss     &                               \\ \cmidrule(r){1-3} \cmidrule(l){8-8} 
\multirow{3}{*}{Linear FFN} & \multirow{3}{*}{84640}     & MNIST                    &                  &              &               &          &                               \\
                            &                            & FashionMNIST             &                  &              &               &          &                               \\
                            &                            & CIFAR-10                 &                  &              &               &          &                               \\
                            &                            &                          &                  &              &               &          &                               \\
GSC                         & 132100                     & MNIST                    &                  &              &               &          &                               \\
                            &                            & FashionMNIST             &                  &              &               &          &                               \\
                            &                            & CIFAR-10                 &                  &              &               &          &                               \\
                            &                            &                          &                  &              &               &          &                               \\
DSC                         & 133034                     & MNIST                    &                  &              &               &          &                               \\
                            &                            & FashionMNIST             &                  &              &               &          &                               \\
                            &                            & CIFAR-10                 &                  &              &               &          &                               \\
                            &                            &                          &                  &              &               &          &                               \\
DSC-10p                     & 24654                      & MNIST                    &                  &              &               &          &                               \\
                            &                            & FashionMNIST             &                  &              &               &          &                               \\
                            &                            & CIFAR-10                 &                  &              &               &          &                               \\ \bottomrule
\end{tabular}
\caption{100\% Data}
\label{tab:my-table}
\end{table}



\begin{figure}[h!]
\centering
\includegraphics[scale=0.08]{InfoTransfer.png}
\caption{Information Transfer}
\label{fig:infotransfer}
\end{figure}


\subsection{Lists}
\begin{itemize}
\item Lorem ipsum dolor sit amet
\item consectetur adipiscing elit. 
\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\section{Conclusion}



\bibliographystyle{unsrt}  
\bibliography{references}
%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.
% \begin{thebibliography}{1}

% \bibitem{kour2014real}
% George Kour and Raid Saabne.
% \newblock Real-time segmentation of on-line handwritten arabic script.
% \newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
%   International Conference on}, pages 417--422. IEEE, 2014.

% \bibitem{kour2014fast}
% George Kour and Raid Saabne.
% \newblock Fast classification of handwritten on-line arabic characters.
% \newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
%   International Conference of}, pages 312--318. IEEE, 2014.

% \bibitem{hadash2018estimate}
% Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
%   Jacovi.
% \newblock Estimate and replace: A novel approach to integrating deep neural
%   networks with existing applications.
% \newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}


